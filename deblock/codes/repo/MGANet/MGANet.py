import torch
import torch.nn as nn
from torch.nn.init import kaiming_normal
import repo.MGANet.BiConvLSTM as BiConvLSTM
from models.base_model import BaseModel


def conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1):
    if batchNorm:
        return nn.Sequential(
            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2,
                      bias=False),
            nn.BatchNorm2d(out_planes),
            nn.LeakyReLU(0.05, inplace=True)
        )
    else:
        return nn.Sequential(
            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2,
                      bias=True),
            nn.LeakyReLU(0.05, inplace=True)
        )


def conv_no_lrelu(batchNorm, in_planes, out_planes, kernel_size=3, stride=1):
    if batchNorm:
        return nn.Sequential(
            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2,
                      bias=False),
            nn.BatchNorm2d(out_planes)
        )
    else:
        return nn.Sequential(
            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2,
                      bias=True),
        )


def predict_image(in_planes):
    return nn.Conv2d(in_planes, 1, kernel_size=3, stride=1, padding=1, bias=False)


def deconv(in_planes, out_planes):
    return nn.Sequential(
        nn.ConvTranspose2d(in_planes, out_planes, kernel_size=4, stride=2, padding=1, bias=False),
        nn.LeakyReLU(0.05, inplace=True)
    )


def crop_like(input, target):
    if input.size()[2:] == target.size()[2:]:
        return input
    else:
        return input[:, :, :target.size(2), :target.size(3)]


class Gen_Guided_UNet(BaseModel):
    expansion = 1

    def __init__(self, input_size):
        super(Gen_Guided_UNet, self).__init__()
        self.input_size = input_size
        self.batchNorm = False
        self.pre_conv1 = conv(self.batchNorm, 1, 64, kernel_size=3, stride=1)
        self.pre_conv1_1 = conv(self.batchNorm, 64, 64, kernel_size=3, stride=1)

        self.pre_conv2 = conv(self.batchNorm, 1, 64, kernel_size=3, stride=1)
        self.pre_conv2_1 = conv(self.batchNorm, 64, 64, kernel_size=3, stride=1)

        self.pre_conv3 = conv(self.batchNorm, 1, 64, kernel_size=3, stride=1)
        self.pre_conv3_1 = conv(self.batchNorm, 64, 64, kernel_size=3, stride=1)

        self.biconvlstm = BiConvLSTM(input_size=(self.input_size[0], self.input_size[1]), input_dim=64, hidden_dim=64,
                                     kernel_size=(3, 3), num_layers=1)

        self.LSTM_out = conv(self.batchNorm, 128, 64, kernel_size=1, stride=1)

        # self.conv1_mask = conv(self.batchNorm, 1, 64, kernel_size=3, stride=1)
        # self.conv2_mask = conv(self.batchNorm, 64, 64, kernel_size=3, stride=1)

        self.conv1 = conv(self.batchNorm, 64, 128, kernel_size=7, stride=2)  # 64
        self.conv1_1 = conv(self.batchNorm, 128, 128)  # 128*128 ->64*64
        self.conv2 = conv(self.batchNorm, 128, 256, kernel_size=3, stride=2)  # 64 ->32
        self.conv2_1 = conv(self.batchNorm, 256, 256)  # 128*128 ->64*64
        self.conv3 = conv(self.batchNorm, 256, 512, kernel_size=3, stride=2)  # 32->16
        self.conv3_1 = conv(self.batchNorm, 512, 512)
        self.conv4 = conv(self.batchNorm, 512, 1024, kernel_size=3, stride=2)  # 16->8
        self.conv4_1 = conv(self.batchNorm, 1024, 1024)

        self.deconv4 = deconv(1024, 512)
        self.deconv3 = deconv(1025, 256)
        self.deconv2 = deconv(513, 128)
        self.deconv1 = deconv(257, 64)

        self.predict_image4 = predict_image(1024)
        self.predict_image3 = predict_image(1025)
        self.predict_image2 = predict_image(513)
        self.predict_image1 = predict_image(257)

        self.upsampled_image4_to_3 = nn.ConvTranspose2d(1, 1, 4, 2, 1, bias=False)  # 8_16
        self.upsampled_image3_to_2 = nn.ConvTranspose2d(1, 1, 4, 2, 1, bias=False)  # 16-32
        self.upsampled_image2_to_1 = nn.ConvTranspose2d(1, 1, 4, 2, 1, bias=False)  # 32-64
        self.upsampled_image1_to_finally = nn.ConvTranspose2d(1, 1, 4, 2, 1, bias=False)  # 64-128

        self.output1 = conv(self.batchNorm, 129, 64, kernel_size=3, stride=1)
        self.output2 = conv(self.batchNorm, 64, 64, kernel_size=3, stride=1)
        self.output3 = conv_no_lrelu(self.batchNorm, 64, 1, kernel_size=3, stride=1)

        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
                kaiming_normal(m.weight.data, a=0.05)
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def forward(self, data1, data2, data3):

        CNN_seq = []
        pre_conv1 = self.pre_conv1(data1)
        pre_conv1_1 = self.pre_conv1_1(pre_conv1)
        CNN_seq.append(pre_conv1_1)

        pre_conv2 = self.pre_conv2(data2)
        pre_conv2_1 = self.pre_conv2_1(pre_conv2)
        CNN_seq.append(pre_conv2_1)

        pre_conv3 = self.pre_conv3(data3)
        pre_conv3_1 = self.pre_conv3_1(pre_conv3)
        CNN_seq.append(pre_conv3_1)

        CNN_seq_out = torch.stack(CNN_seq, dim=1)
        CNN_seq_feature_maps = self.biconvlstm(CNN_seq_out)
        # CNN_concat_input = CNN_seq_out[:, 1, ...]+CNN_seq_feature_maps[:, 1, ...]
        CNN_concat_input = torch.cat([CNN_seq_out[:, 1, ...], CNN_seq_feature_maps[:, 1, ...]], dim=1)

        LSTM_out = self.LSTM_out(CNN_concat_input)  # 128*128*64

        # conv1_mask = self.conv1_mask(mask)
        # conv2_mask = self.conv2_mask(conv1_mask)  # 128*128*64

        out_conv1 = self.conv1_1(self.conv1(LSTM_out))
        out_conv2 = self.conv2_1(self.conv2(out_conv1))
        out_conv3 = self.conv3_1(self.conv3(out_conv2))
        out_conv4 = self.conv4_1(self.conv4(out_conv3))

        # out_conv1_mask = self.conv1_1(self.conv1(conv2_mask))
        # out_conv2_mask = self.conv2_1(self.conv2(out_conv1_mask))
        # out_conv3_mask = self.conv3_1(self.conv3(out_conv2_mask))
        # out_conv4_mask = self.conv4_1(self.conv4(out_conv3_mask))

        # sum4 = out_conv4 + out_conv4_mask
        sum4 = out_conv4
        image_4 = self.predict_image4(sum4)
        image_4_up = crop_like(self.upsampled_image4_to_3(image_4), out_conv3)
        out_deconv3 = crop_like(self.deconv4(sum4), out_conv3)

        # sum3 = out_conv3 + out_conv3_mask
        sum3 = out_conv3
        concat3 = torch.cat((sum3, out_deconv3, image_4_up), dim=1)
        image_3 = self.predict_image3(concat3)
        image_3_up = crop_like(self.upsampled_image3_to_2(image_3), out_conv2)
        out_deconv2 = crop_like(self.deconv3(concat3), out_conv2)

        # sum2 = out_conv2 + out_conv2_mask
        sum2 = out_conv2
        concat2 = torch.cat((sum2, out_deconv2, image_3_up), dim=1)
        image_2 = self.predict_image2(concat2)
        image_2_up = crop_like(self.upsampled_image2_to_1(image_2), out_conv1)
        out_deconv2 = crop_like(self.deconv2(concat2), out_conv1)

        # sum1 = out_conv1 + out_conv1_mask
        sum1 = out_conv1
        concat1 = torch.cat((sum1, out_deconv2, image_2_up), dim=1)
        image_1 = self.predict_image1(concat1)
        image_1_up = crop_like(self.upsampled_image1_to_finally(image_1), LSTM_out)
        # print(image_1_up.shape)
        out_deconv1 = crop_like(self.deconv1(concat1), LSTM_out)

        # sum0 = LSTM_out + conv2_mask
        sum0 = LSTM_out
        concat0 = torch.cat([sum0, out_deconv1, image_1_up], dim=1)
        image_out = self.output1(concat0)
        image_out2 = self.output2(image_out)
        image_finally = self.output3(image_out2)

        image_finally = torch.clamp(image_finally, 0., 1.)
        # print('image_1',image_finally.shape)

        return image_finally
